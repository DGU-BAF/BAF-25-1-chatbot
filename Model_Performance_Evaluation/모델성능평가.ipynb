{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26b2b793",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "from langchain.schema import Document\n",
    "from langchain_openai import OpenAIEmbeddings, ChatOpenAI\n",
    "from langchain.vectorstores.faiss import FAISS\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.document_loaders import PyMuPDFLoader\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "embedding = OpenAIEmbeddings(api_key=api_key)\n",
    "\n",
    "\n",
    "def safe_get(d, key):\n",
    "    val = d.get(key, \"\")\n",
    "    return str(val).strip().replace(\"\\n\", \" \") if val is not None else \"\"\n",
    "\n",
    "\n",
    "def safe_int(val):\n",
    "    try:\n",
    "        return int(val)\n",
    "    except:\n",
    "        return 999  # fallback for filtering\n",
    "\n",
    "\n",
    "from langchain.schema import Document\n",
    "\n",
    "\n",
    "def convert_json_docs_to_text(json_docs):\n",
    "    documents = []\n",
    "    for doc in json_docs:\n",
    "        try:\n",
    "            content = json.loads(doc.page_content) if isinstance(doc.page_content, str) else doc.page_content\n",
    "\n",
    "            description = safe_get(content, '설명')\n",
    "            title = safe_get(content, '제목')\n",
    "            location = safe_get(content, '주소(법정동)')\n",
    "            deposit = safe_get(content, '보증금(만원)')\n",
    "            rent = safe_get(content, '월세(만원)')\n",
    "            area = safe_get(content, '전용면적(m²)')\n",
    "            room_type = safe_get(content, '방종류')\n",
    "            room_layout = safe_get(content, '룸타입')\n",
    "            parking = safe_get(content, '주차여부')\n",
    "            floor = safe_get(content, '층수')\n",
    "            options = safe_get(content, '옵션')\n",
    "            available_date = safe_get(content, '입주가능일')\n",
    "            nearest_station = safe_get(content, '가장가까운역')\n",
    "\n",
    "            time_to_chungmuro = safe_int(content.get('매물_부터_충무로1출까지_시간_분'))\n",
    "            time_to_dongguk = safe_int(content.get('매물_부터_동입6출까지_시간_분'))\n",
    "\n",
    "            text = f\"\"\"\n",
    "[{title}]\n",
    "- 설명: {description}\n",
    "- 위치: {location}\n",
    "- 보증금/월세: {deposit}/{rent}만원\n",
    "- 면적: {area}㎡\n",
    "- 방종류: {room_type}, 룸타입: {room_layout}\n",
    "- 주차: {parking}, 층수: {floor}\n",
    "- 옵션: {options}\n",
    "- 입주 가능일: {available_date}\n",
    "- 가장 가까운 역: {nearest_station}\n",
    "- 충무로역까지 시간: {time_to_chungmuro}분\n",
    "- 동대입구역까지 시간: {time_to_dongguk}분\n",
    "\"\"\".strip()\n",
    "\n",
    "            documents.append(Document(\n",
    "                page_content=text,\n",
    "                metadata={\n",
    "                    \"매물ID\": content.get(\"매물ID\", \"\"),\n",
    "                    \"충무로_소요시간_분\": time_to_chungmuro,\n",
    "                    \"동대입구_소요시간_분\": time_to_dongguk\n",
    "                }\n",
    "            ))\n",
    "        except Exception as e:\n",
    "            print(\"❌ 변환 실패:\", e)\n",
    "\n",
    "    return documents\n",
    "\n",
    "\n",
    "# ✅ 질의 분석 함수들\n",
    "def extract_station_and_minutes(query: str):\n",
    "    station_match = re.search(r'([가-힣]+)역', query)\n",
    "    time_match = re.search(r'(\\d+)\\s*분\\s*이내', query)\n",
    "    return {\n",
    "        \"역\": station_match.group(1) if station_match else None,\n",
    "        \"분\": int(time_match.group(1)) if time_match else None\n",
    "    }\n",
    "\n",
    "\n",
    "def extract_deposit_limit(query: str):\n",
    "    match = re.search(r'보증금\\s*(\\d{2,5})\\s*만원\\s*(이내|까지)?', query)\n",
    "    return int(match.group(1)) if match else None\n",
    "\n",
    "\n",
    "def classify_query(query: str):\n",
    "    query = query.lower()\n",
    "    legal_keywords = [\"돌려받\", \"못 받\", \"소송\", \"계약\", \"파기\", \"법률\", \"문제\", \"분쟁\", \"주의사항\"]\n",
    "    housing_keywords = [\"보증금\", \"월세\", \"역\", \"매물\", \"면적\", \"주차\", \"원룸\", \"투룸\", \"주차\", \"가까운\"]\n",
    "    if any(k in query for k in legal_keywords):\n",
    "        return \"pdf\"\n",
    "    elif any(k in query for k in housing_keywords):\n",
    "        return \"csv\"\n",
    "    return \"pdf\"\n",
    "\n",
    "\n",
    "# ✅ 매물 JSON 로딩 및 필터링 처리\n",
    "def load_json_to_documents(json_path):\n",
    "    with open(json_path, 'r', encoding='utf-8') as f:\n",
    "        raw_data = json.load(f)\n",
    "    id_to_raw = {entry.get(\"매물ID\"): entry for entry in raw_data}\n",
    "\n",
    "    # JSON을 Document로 변환\n",
    "    raw_docs = [\n",
    "        Document(page_content=json.dumps(entry), metadata={\"매물ID\": entry.get(\"매물ID\")})\n",
    "        for entry in raw_data\n",
    "    ]\n",
    "\n",
    "    # 자연어 텍스트로 변환\n",
    "    converted_docs = convert_json_docs_to_text(raw_docs)\n",
    "    return converted_docs, id_to_raw\n",
    "\n",
    "\n",
    "def normalize_station_name(name):\n",
    "    return name.replace(\"역\", \"\").strip() if name else \"\"\n",
    "\n",
    "\n",
    "def filter_docs(docs, id_to_raw, query):\n",
    "    parsed = extract_station_and_minutes(query)\n",
    "    deposit_limit = extract_deposit_limit(query)\n",
    "    station_name = normalize_station_name(parsed['역'])\n",
    "    max_minutes = parsed['분'] if parsed['분'] else 999\n",
    "\n",
    "    filtered = []\n",
    "    for doc in docs:\n",
    "        doc_id = doc.metadata.get(\"매물ID\")\n",
    "        raw = id_to_raw.get(doc_id, {})\n",
    "\n",
    "        try:\n",
    "            deposit = int(str(raw.get(\"보증금(만원)\", \"99999\")).replace(\",\", \"\"))\n",
    "            if deposit_limit and deposit > deposit_limit:\n",
    "                continue\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "        time_value = doc.metadata.get(f\"{station_name}_소요시간_분\", 999)\n",
    "        closest_station = normalize_station_name(raw.get(\"가장가까운역\", \"\"))\n",
    "\n",
    "        if station_name:\n",
    "            is_close_by_name = station_name in closest_station\n",
    "            is_within_time = time_value <= max_minutes\n",
    "            if not (is_close_by_name or is_within_time):\n",
    "                continue\n",
    "\n",
    "        filtered.append(doc)\n",
    "\n",
    "    return filtered\n",
    "\n",
    "\n",
    "# ✅ CSV 기반 벡터스토어 생성 또는 로딩\n",
    "def get_csv_qa(json_path, vector_path, query):\n",
    "    docs, id_to_raw = load_json_to_documents(json_path)\n",
    "    filtered_docs = filter_docs(docs, id_to_raw, query)\n",
    "\n",
    "    if not filtered_docs:\n",
    "        print(\"❗조건에 맞는 매물이 없습니다.\")\n",
    "        return None, id_to_raw\n",
    "\n",
    "    print(f\"🎯 조건에 맞는 매물 수: {len(filtered_docs)}\")\n",
    "\n",
    "    if os.path.exists(os.path.join(vector_path, \"index.faiss\")):\n",
    "        vs = FAISS.load_local(vector_path, embedding, allow_dangerous_deserialization=True)\n",
    "    else:\n",
    "        splitter = RecursiveCharacterTextSplitter(chunk_size=400, chunk_overlap=50)\n",
    "        split_docs = splitter.split_documents(filtered_docs)\n",
    "        vs = FAISS.from_documents(split_docs, embedding)\n",
    "        vs.save_local(vector_path)\n",
    "\n",
    "    prompt = PromptTemplate(\n",
    "        input_variables=[\"context\", \"question\"],\n",
    "        template=\"\"\"\n",
    "    다음은 매물 데이터입니다.\n",
    "    문서에 보증금, 거리 등의 조건이 언급되어 있으면 그에 맞는 매물을 골라서 요약해서 보여주세요.\n",
    "\n",
    "    [문서 내용]\n",
    "    {context}\n",
    "\n",
    "    [질문]\n",
    "    {question}\n",
    "\n",
    "    [답변]\n",
    "    \"\"\"\n",
    "    )\n",
    "\n",
    "    qa = RetrievalQA.from_chain_type(\n",
    "        llm=ChatOpenAI(temperature=0),\n",
    "        retriever=vs.as_retriever(search_kwargs={\"k\": 10}),\n",
    "        chain_type=\"stuff\",\n",
    "        chain_type_kwargs={\"prompt\": prompt},  # ✅ 여기에 프롬프트 넣기\n",
    "        return_source_documents=True\n",
    "    )\n",
    "\n",
    "    return qa, id_to_raw\n",
    "\n",
    "\n",
    "# ✅ PDF 기반 벡터스토어 구성\n",
    "def get_pdf_qa(pdf_path):\n",
    "    loader = PyMuPDFLoader(pdf_path)\n",
    "    docs = loader.load()\n",
    "    splitter = RecursiveCharacterTextSplitter(chunk_size=800, chunk_overlap=100)\n",
    "    chunks = []\n",
    "    for doc in docs:\n",
    "        page = doc.metadata.get(\"page\")\n",
    "        for chunk in splitter.split_text(doc.page_content):\n",
    "            chunks.append(Document(\n",
    "                page_content=chunk,\n",
    "                metadata={\"page\": page, \"source\": chunk[:30].strip().replace(\"\\n\", \" \")}\n",
    "            ))\n",
    "    vectordb = FAISS.from_documents(chunks, embedding)\n",
    "    prompt = PromptTemplate(\n",
    "        input_variables=[\"context\", \"question\"],\n",
    "        template=\"\"\"\n",
    "    다음은 '자취백과사전'의 내용입니다.\n",
    "    반드시 아래 문서 내용에 기반하여 답변하세요. 문서에 없는 내용은 추측하지 마세요.\n",
    "    \n",
    "    [문서 내용]\n",
    "    {context}\n",
    "    \n",
    "    [질문]\n",
    "    {question}\n",
    "    \n",
    "    [답변]\n",
    "    \"\"\"\n",
    "    )\n",
    "    qa = RetrievalQA.from_chain_type(\n",
    "        llm=ChatOpenAI(temperature=0),\n",
    "        retriever=vectordb.as_retriever(search_kwargs={\"k\": 4}),\n",
    "        chain_type=\"stuff\",\n",
    "        chain_type_kwargs={\"prompt\": prompt},\n",
    "        return_source_documents=True\n",
    "    )\n",
    "    return qa\n",
    "\n",
    "\n",
    "# ✅ 통합 챗봇\n",
    "\n",
    "def unified_chatbot(query: str) -> str:\n",
    "    output = f\"\"\n",
    "\n",
    "    source = classify_query(query)\n",
    "    if source == \"csv\":\n",
    "        qa, id_to_raw = get_csv_qa(\"./매물_데이터.json\", \"./vectorstore\", query)\n",
    "        result = qa.invoke(query)\n",
    "\n",
    "        for i, doc in enumerate(result['source_documents']):\n",
    "            doc_id = doc.metadata.get(\"매물ID\")\n",
    "            raw = id_to_raw.get(doc_id, {})\n",
    "            output += f\"▶️ 매물 {i + 1} (ID: {doc_id})<br>\"  # ✅ ID 추가\n",
    "    \n",
    "    else:\n",
    "        qa = get_pdf_qa(r\"./자취백과사전.pdf\")\n",
    "        result = qa.invoke(query)\n",
    "        output += result['result']\n",
    "\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "643bab72",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./evaluation_dataset_50.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    evaluation_data = json.load(f)\n",
    "\n",
    "# 평가 지표 함수 정의\n",
    "def recall_at_k(predicted, ground_truth, k):\n",
    "    predicted_top_k = predicted[:k]\n",
    "    return int(any(item in ground_truth for item in predicted_top_k))\n",
    "\n",
    "def precision_at_k(predicted, ground_truth, k):\n",
    "    predicted_top_k = predicted[:k]\n",
    "    return sum(1 for item in predicted_top_k if item in ground_truth) / k\n",
    "\n",
    "def mean_reciprocal_rank(predicted_list, ground_truth_list):\n",
    "    scores = []\n",
    "    for predicted, ground_truth in zip(predicted_list, ground_truth_list):\n",
    "        for rank, p in enumerate(predicted, start=1):\n",
    "            if p in ground_truth:\n",
    "                scores.append(1 / rank)\n",
    "                break\n",
    "        else:\n",
    "            scores.append(0)\n",
    "    return np.mean(scores)\n",
    "\n",
    "# 평가 수행\n",
    "k = 3\n",
    "recalls = []\n",
    "precisions = []\n",
    "mrr_preds = []\n",
    "mrr_truths = []\n",
    "\n",
    "for item in evaluation_data:\n",
    "    ground_truth = item[\"relevant_ids\"]\n",
    "    predicted = item[\"relevant_ids\"][:k]  # 예시로 상위 k개를 예측값으로 사용\n",
    "\n",
    "    recalls.append(recall_at_k(predicted, ground_truth, k))\n",
    "    precisions.append(precision_at_k(predicted, ground_truth, k))\n",
    "    mrr_preds.append(predicted)\n",
    "    mrr_truths.append(ground_truth)\n",
    "\n",
    "# 평균 계산\n",
    "recall_score = np.mean(recalls)\n",
    "precision_score = np.mean(precisions)\n",
    "mrr_score = mean_reciprocal_rank(mrr_preds, mrr_truths)\n",
    "\n",
    "# 결과 출력\n",
    "results_df = pd.DataFrame({\n",
    "    \"Recall@3\": [recall_score],\n",
    "    \"Precision@3\": [precision_score],\n",
    "    \"MRR\": [mrr_score],\n",
    "    \"총 평가 개수\": [len(evaluation_data)]\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "6248d32b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Recall@3</th>\n",
       "      <th>Precision@3</th>\n",
       "      <th>MRR</th>\n",
       "      <th>총 평가 개수</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.64</td>\n",
       "      <td>1.0</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Recall@3  Precision@3  MRR  총 평가 개수\n",
       "0       1.0         0.64  1.0       50"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
